---
title: "Yanny-Laurel Analysis"
output:
  html_document:
    df_print: paged
---

# Demo Experiment Analysis

## Getting Start: R Markdown

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

---

## 1a. Setup
Hello!

Let's get started. You should have downloaded the Yanny/Laurel demo repository from Github, but if not you can do so now via [this link](https://github.com/avakiai/yanny-laurel-demo). 
Save it someplace convenient. 
If you already have the repository downloaded, you can make sure you have this up-to-date file; you can either do this by downloading this individual script as well as the file ```helper_functions.R``` from the ```2_analysis``` folder, or using git (if you are familiar with it, otherwise no stress!).

First, set working directory to where this file is by going to the ```Session``` tab, then ```Set Workspace Directory > To Source File Location```.

Something like this should show up in your console: ```setwd("[wherever you saved the repository]/yanny-laurel-demo/2_analysis")```

We like to use relative paths instead of absolute paths. Absolute paths are directories
that are specific to your computer. For example, this file is located on my computer
by the following path: ```setwd("~/Repos/yanny-laurel-demo/2_analysis")```

But if I put that in the code, then it would fail when you try to run it because that
path probably does not exist on your computer. 

A much better (i.e. more reproducible and resilient) way of coding is to organize
your project so that you or anyone else can simply set the working directory to the
folder where the analysis scripts are (or the main folder of the directory), and 
"access" everything else from there. 

**Note**: You can simply run the following chunks of code up to [Section 2](## 2. Exploring the data), where the interactive part starts. 

Code where you have to fill in some parts to run it are commented out, uncomment them first and then proceed! :)

### Set paths
So what would this look like? 
Like so: 
```{r}
# set working directory to source file location
data_path = file.path('../data') 
results_path = file.path('../3_results')
# the '.' means "working directory"
# the '/' means go down one folder level
# the '../' means go up one folder level

```
So since we are setting the working directory to this file, we have to go up one
level (to the main folder), then down to access the 'data' and 'results' folders. 

### Load Packages & Functions
```{r warning=FALSE}
library(tidyverse)
# Helper functions
source('./helper_functions.R')
```

### Load Data
Now let's load and inspect the data...
```{r}
# first let's check to make sure we're in the right place:
files <- list.files(data_path, pattern = ".csv")

# Note that putting () around a command will cause it to print the result directly to the console. If you assign a variable without using (), it will silently perform the assignment, and you'd have to run the variable itself to see its contents in the console. You don't need () if you are just running a command without assigning it to anything. 

# now load
data_raw <- data.frame() 
# add blocking
for (f in files) {
  incoming <- read.csv(file.path(data_path, f))
    breaks <- which(!is.na(incoming$block.thisRepN))
    bks <- length(breaks)
    ns <- c(breaks[1],diff(breaks))-1
  cleaned <- incoming[!is.na(incoming$`report.response`),]  
  cleaned$block <- rep_block(bks,ns)
  data_raw <- rbind(data_raw,cleaned) 
}

data_raw
```

You'll notice we cannot immediately start analyzing this data.
There's information that's redundant (like the OS) and the automatically generated variable
names are not really useful or informative. 

Once we have an idea what these mean, and we decide what variables we want to keep, we can 
"wrangle" the data to get it into the format we need. 

## 1b. Wrangle Data
Note that you almost certainly will have to do data wrangling several times in the course
of an analysis, as different visualization and statistical methods require different formats
for the data. 

Let's start by cleaning up the data a bit, knowing a few things about our experiment:

1. We know that the slider used to get responses (called `report`) in each trial
shows up 0.2 seconds after trial start, but the audio stimulus is played 0.5 s after start. 
Since the reaction time on the `report` is calculated relative to the start of that 
component, we have to subtract 0.3 from the reaction time to get a meaningful reaction
time - one that is relative to the onset of the auditory stimulus.

2. In Python, indexing starts at 0 (e.g. the first element of a list is element `0`, then `1`, etc.) so you might want to re-number trial values to be a bit more intuitive. 

3. Our slider had been programmed to have two "tick marks," 1 = "Laurel" and 2 = "Yanny."
Since we want to get the proportion of "Yanny" responses, we should rescale the responses so that
0 = "Laurel" and 1 = "Yanny". Then, a simple mean over trials would give us the % of trials 
on which participants heard "Yanny."

```{r}
data_orig <- data_raw %>% dplyr::rename(music_yrs = years.musical.experience, # rename variables
                                    resp = report.response, # whether they responded yanny (2) or laurel (1)
                                    RT = report.rt, # reaction time
                                    trial_rep_n = trials.thisRepN, # whether its the 1st or 2nd time this token
                                                                   # was played in this block
                                    trial_n = trials.thisN, # occurence in the block
                                    stim_idx = trials.thisIndex, # stimulus identifier
                                    stim = audio) %>% # stimulus filename
                      # pick which vars to keep, and in which order
                      dplyr::select(participant, sex, age, music_yrs, # pariticpant variables
                                    block, trial_n, stim, stim_idx, trial_rep_n, # independent variables
                                    resp, RT) %>% # dependent variables
                      # get data into format we'd like
                      dplyr::mutate(trial_n = trial_n+1, # rescale to adjust for Python indexing
                                    stim_idx = stim_idx+1,
                                    trial_rep_n = trial_rep_n+1,
                                    resp = round(resp)-1, # convert 1,2 responses to 0,1
                                    RT = RT-0.3) %>% # adjust for trial timing
                      dplyr::mutate(participant = as.factor(participant), # code as categorical variables
                                    sex = as.factor(sex))

# make sure you have complete data for all observations
nrow(data_orig)      
nrow(data_orig %>% na.omit()) # see also dplyr::complete.cases

data <- data_orig %>%
  dplyr::mutate(dB_ratio = ordered(stim_idx, labels = c(-60,-48,-36,-24,-12,0,12,24,36,48,60))) # make a copy
# data
#data$participant %>% unique()

write.csv(data, file.path(data_path,"data.csv"))
```


## 2. Exploring the data

Use ```summary`` to get an overview of the data and variables...
```{r}
summary(data)
```
What are we looking at here? Let's explore some of these variables...

First let's look at the participant variables. What is the demographic breakdown of our dataset? 
```{r}
hist(data$age) 

age_dat <- data %>% dplyr::group_by(participant) %>% summarise(age = unique(age))
 
hist(age_dat$age)

 #???(data$sex)
```
Here, it's counting up all the observations. How can we get it to count only the unique occurences?
 
See above!
```{r}
#???(data$age)

```

Okay, let's move on to our IV & DV's. We're going to focus on the proportion of trials in which participants responded 'Yanny'.

```{r}
hist(data$resp)
```
What we actually want to know is how people's responses varied based on the level of acoustic manipulation, indexed by ```stim_idx```. We also want a summary based on each participant. So: 

#### Calculate Proportion Yanny
```{r message=FALSE, warning=FALSE}
prop_data <- data %>% dplyr::group_by(participant, dB_ratio) %>% 
                      dplyr::summarise(prop_yanny = mean(resp),
                                       median_RT = median(RT),
                                       iqr_RT = IQR(RT),
                                       mad_RT = mad(RT))
prop_data
```

Plot... 
```{r warning=FALSE}
ggplot(data = prop_data, mapping = aes(dB_ratio, prop_yanny, color = participant)) +
  geom_point() + 
  geom_line(mapping = aes(group = participant)) + 
  scale_y_continuous(name = "Prop. 'Yanny'", limits = c(0,1)) +
  scale_x_discrete(name = "High/low ratio (dB)") + 
  theme_classic()
```

## 4. Aggregating Data & Plotting

Let's look at how the levels of the low/high frequency dB ratio affected the mean proportion of "Yanny" (vs. "Laurel") responses, across all participants/blocks/trials...

```{r}
data_agg <- aggregate(resp ~ dB_ratio, data=data, FUN=mean)

data_agg
```

```{r}
group_means <- aggregate(prop_yanny~dB_ratio,data=prop_data, FUN=mean)

ggplot(data = prop_data, mapping = aes(dB_ratio, prop_yanny, color = participant)) +
  geom_point() + 
  geom_line(mapping = aes(group = participant)) + 
  geom_line(data = group_means, aes(dB_ratio, prop_yanny, group = 1), color = "black", size = 2) + 
  scale_y_continuous(name = "Prop. 'Yanny'", limits = c(0,1)) +
  scale_x_discrete(name = "High/low ratio (dB)") + 
  theme_classic()
```

Suppose you wanted to aggregate the data so that you saw the effect of age, sex, and years of musical experience. How would you do it?
```{r message=FALSE, warning=FALSE}

prop_data_demog <- aggregate(resp ~ sex, data = data, FUN=mean)

prop_data_demo2 <- data %>% dplyr::group_by(sex, dB_ratio) %>% 
                      dplyr::summarise(prop_yanny = mean(resp),
                                       sd = sd(resp),
                                       median_RT = median(RT),
                                       iqr_RT = IQR(RT),
                                       mad_RT = mad(RT))
```

How might you plot this?

```{r}
# ggplot(data = ???, mapping = aes(dB_ratio, prop_yanny, color = ???)) +
#  geom_point() +
#  geom_line(mapping = aes(group = ???)) +
#  scale_y_continuous(name = "Prop. 'Yanny'", limits = c(0,1)) +
#  scale_x_discrete(name = "High/low ratio (dB)") +
#  theme_classic()
```


### Logistic Regression
We are interested in the effects of the dB ratio level on perception of Yanny/Laurel...
Since we're interested in the likelihood of participant choosing one of two categorical variables, we can model this as a binomial ("two-variable") logistic regression. 

You can read more about logistic regression [here](https://uc-r.github.io/logistic_regression#eval), and in many other places online. 

We will first go the simplest route, using only the low/high dB ratio as a predictor:
```{r}
model1 <- glm(resp ~ stim_idx, family = binomial(link='logit'), data = data)

summary(model1)

saveRDS(model1, file.path(results_path,"model.rds"))
```
The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable.

* For every one unit change in the dB ratio, the log odds of hearing Yanny (versus Laurel) increases by 0.623.

#### Odd Ratio & Confidence Intervals

Usually, the results of a logistic regression are expressed as an "odds ratio." To get this, you need to basically remove the "log" part of the "log odd" interpretation of the coefficients. So, you exponentiate the coefficients!

To get the confidence intervals we use the ```confint()``` function...

```{r}
coef(model1)

# odds ratio
exp(coef(model1))

exp( cbind(odd_ratio = coef(model1), confint(model1,level = .95)) )
```
Now we can say that for a one unit increase in the stimulus, the odds of hearing Yanny increase by a factor of about 1.87. Note that the odds ratio for the intercept is not generally interpreted.

Note! Mathematically, probability and odds ratio are two different things. Probability is the likelihood that an event will occur. Odds ratio is the likelihood that an event will occur in relation to the likelihood that an event will not occur: 

$odds = p(event)/ p(!event)$, where "!" means "not"

That means than an odds ratio > 1 indicates an increased likelihood of the event occuring, while an OR < 1 indicates a decreased likelihood. 

Take a look again at the table we generated above. How would you read this in plain language?

### Multiple Logistic Regression
Now suppose we also want to model the other variables in our dataset, such as age, sex and years of musical experience. How would we do that? 

Give it a try, then go ahead and calculate the odd ratio and confidence intervals.

```{r}
model2 <- glm(resp ~ stim_idx + age + sex + music_yrs, family = binomial(link='logit'), data = data)
 
summary(model2)


anova(model2, test = "Chisq")
```

```{r}
model3 <- glm(resp ~ stim_idx + age + music_yrs, family = binomial(link='logit'), data = data)
 
summary(model3)

anova(model3, test = "Chisq")

model4 <- glm(resp ~ stim_idx + age, family = binomial(link='logit'), data = data)
summary(model4)

anova(model4, test = "Chisq")
```

#### Model Comparison
```{r}
anova(model1, model2, model3, test="Chisq")
```

The deviance tells us how well our model is doing against a "null" model (a model with only the intercept). It's a measure of how well our model fits the data. 

Keep in mind what the chi-square test is measuring:
$$\chi^2 = \sum \frac {(Observed - Expected)^2}{Expected}$$

### Writing it up... 
There's no clear format for reporting the results of a logistic regression in APA format (as far as I know), but most often you will see people report the odds ratios and CIs, as well as indicate which coefficients/variables were significant. 

Thus, for the univariate logistic regression, we might write: 

"The results of the binomial logistic regression indicated that there was a significant association between the low/high dB ratio and the probability of hearing "Yanny" over "Laurel". The odds ratio was 1.87, and the 95% confidence interval suggested the true value of the population odds lies between 1.69 and 2.09."

#### Predicted Values
Note that if we add the argument ```type = "response"``` to the function call, the ```predict()``` function returns the probability and not the log-odds. 
```{r}
pred <- data.frame(p_pred = predict(model4, data, type="response"), 
                   dB_ratio = data$dB_ratio,
                   age = data$age)  # predicted scores

ggplot() +
  geom_point(data = data, mapping = aes(dB_ratio, resp)) +
  facet_grid(. ~ sex) + 
  #geom_line(pred, mapping = aes(dB_ratio, p_pred, group = 1)) + 
  scale_y_continuous(name = "Prop. 'Yanny'", limits = c(0,1)) +
  scale_x_discrete(name = "High/low ratio (dB)") + 
  theme_classic()
```

## 4. Analyzing RT

Now try it on your own by analyzing how ```RT``` changes as ```stim_idx``` varies. 
You can follow the steps we went through together:

1. aggregate the data
2. plot the data
3. build a linear model 

As a bonus challenge: How would you report/interpret the results of the linear model? 

# Resources
Some info Logistic Regression in R can be found  [here](https://www.r-bloggers.com/2015/09/how-to-perform-a-logistic-regression-in-r/).

